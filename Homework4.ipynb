{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rofranklin/Homework4_Data_Mining/blob/master/Homework4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "P1g4KnT-lhku",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homework 4"
      ]
    },
    {
      "metadata": {
        "id": "KfA6Z16al4_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.  What are the top 10 common words in the ICML papers?\n",
        "2.  Let Z be a randomly selected word in a randomly selected ICML paper.  Estimate the entropy of Z.\n",
        "3.  Synthesize a random paragraph using the marginal distribution over words.\n",
        "\n",
        "4.  (Extra credit) Synthesize a random paragraph using an n-gram model on words.  Synthesize a random paragraph using any model you want.  Top five synthesized text paragraphs winbonus!"
      ]
    },
    {
      "metadata": {
        "id": "waEBkhynlm-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "d1350c06-6b39-4c19-ced9-e0754bca3ff4"
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib import urlretrieve, urlopen\n",
        "import string\n",
        "from numpy.random import choice\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "#!pip install pdfminer\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from cStringIO import StringIO\n",
        "#!pip install --upgrade setuptools\n",
        "#!apt-get install enchant\n",
        "#!pip install pyenchant\n",
        "import enchant"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfminer in /usr/local/lib/python2.7/dist-packages (20140328)\n",
            "Requirement already up-to-date: setuptools in /usr/local/lib/python2.7/dist-packages (40.9.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "enchant is already the newest version (1.6.0-11.1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Collecting pyenchant\n",
            "  Using cached https://files.pythonhosted.org/packages/9e/54/04d88a59efa33fefb88133ceb638cdf754319030c28aadc5a379d82140ed/pyenchant-2.0.0.tar.gz\n",
            "Building wheels for collected packages: pyenchant\n",
            "  Building wheel for pyenchant (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ee/8e/01/f427e9c6c0ae5e22095f3d2aac8997abe7b317307a9de497f4\n",
            "Successfully built pyenchant\n",
            "Installing collected packages: pyenchant\n",
            "Successfully installed pyenchant-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nb_M0c34ld0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "c961b1d3-df28-480d-fdda-6b3956d674a2"
      },
      "cell_type": "code",
      "source": [
        "dic = enchant.Dict(\"en_US\")\n",
        "htmlContent = requests.get(\"http://proceedings.mlr.press/v80/\").content # Grabbing the website's HTML code\n",
        "\n",
        "content = BeautifulSoup(htmlContent, \"html.parser\") # Parsing our HTML code to make it more workable\n",
        "\n",
        "paperLinks = content.find_all(class_=\"links\") # Grabbing all the page links\n",
        "\n",
        "pdfs = []\n",
        "# Go through each link, grab its href code\n",
        "for link in paperLinks:\n",
        "    allLinks = link.find_all('a') \n",
        "    pdfs.append(allLinks[1].get('href'))\n",
        "    \n",
        "fileNames = []    \n",
        "for i, pdf in enumerate(pdfs[:10]): # For the first 10 pdf links we will actually download the contents , note: to seperate our\n",
        "    content = urlopen(pdf).read()\n",
        "    fileName = os.path.join(os.getcwd(), str(pdf[7:].split('/')[0])) # Make the file name fit our current working directory, split out slashes from original file name and take only part of the name so as to avoid punctiation issues\n",
        "    fileName += str(i) # Add the download number to differentiate pdf names\n",
        "    fileNames.append(fileName)\n",
        "    with open(fileName, 'wb+') as file:\n",
        "        file.write(content) # Put the actual content into this file\n",
        "        file.close()\n",
        "        \n",
        "def convert_pdf_to_text(path):\n",
        "# This code was grabbed from online documentation from pdfminer, turns a pdf to text\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    codec = 'utf-8'\n",
        "    laparams = LAParams()\n",
        "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
        "    fp = open(path,'rb')\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    password = \"\"\n",
        "    maxpages = 0\n",
        "    caching = True\n",
        "    pagenos = set()\n",
        "    \n",
        "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,password = password,caching = caching, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "        \n",
        "    text = retstr.getvalue()\n",
        "    \n",
        "    fp.close()\n",
        "    device.close()\n",
        "    retstr.close()\n",
        "    return text\n",
        "  \n",
        "  \n",
        "totalText = '' \n",
        "# For files in all of our files, combine all of their text\n",
        "for fileName in fileNames:\n",
        "    totalText += convert_pdf_to_text(fileName).lower()\n",
        "\n",
        "words = totalText.split(\" \") # Split each word on spaces\n",
        "words = [word.strip() for word in words]\n",
        "cleanWords = []\n",
        "for word in words: #Check if it is a valid english word, if not - discard it. (Filter out junk words/ data)\n",
        "    try:\n",
        "        if dic.check(word):\n",
        "            cleanWords.append(word)\n",
        "    except: \n",
        "        continue\n",
        "        \n",
        "words = [word for word in cleanWords]\n",
        "\n",
        "#words = [word for word in words if dic.check(word)]\n",
        "\n",
        "uniqueWords = set(words) # Making a set of unique words, note words with punctuation count as different words (we do this so our paragraph will generate random punctuation sometimes)\n",
        "\n",
        "wordCounts = {}\n",
        "\n",
        "for word in uniqueWords: # For each unique word we go through all the papers and count how many times it appears\n",
        "    wordCounts[word] = 0\n",
        "    for w in words:\n",
        "        if w == word:\n",
        "            wordCounts[word] += 1\n",
        "totalCount = len(words)\n",
        "\n",
        "# Calculate probability of each word occuring\n",
        "probs = {} # Dividing the frequency the word appears by the total words in the papaer we get the probability of the word appearing\n",
        "for word in wordCounts:\n",
        "    probs[word] = wordCounts[word] / float(totalCount)\n",
        "    \n",
        "weights = [probs[word] for word in uniqueWords]\n",
        "paragraphLength = random.randint(50,200) # Making a random length for our pararaph generation\n",
        "paragraph = \"\"\n",
        "for i in range(paragraphLength): # Making a weighted random choice from our set of unique words and adding it to our paragraph\n",
        "    if i != 0:\n",
        "        paragraph += \" \"\n",
        "    paragraph += choice(list(uniqueWords),p=weights)\n",
        "print(\"Probabilistically generated paragraph: \")\n",
        "print(paragraph)\n",
        "\n",
        "# Calculate entropy\n",
        "\n",
        "entropy = sum([-probs[word] * math.log(probs[word],2) for word in list(uniqueWords)]) # Entropy function applied to the whole set of unique words and their probabilities\n",
        "print(\"Entropy: \",entropy)\n",
        "\n",
        "# Count the most popular words\n",
        "mostPopularWords = []\n",
        "temp = [word for word in uniqueWords] # We will pop words out of here as we go through our rankings\n",
        "for i in range(10): # Find 10 most popular words\n",
        "    topCount = 0\n",
        "    topWord = \"\"\n",
        "    \n",
        "    for word in temp: # Go through all of the words and find the word with the highest frequency\n",
        "        if wordCounts[word] > topCount: \n",
        "            topWord = word\n",
        "            topCount = wordCounts[word]\n",
        "    temp.remove(topWord)\n",
        "    mostPopularWords.append(topWord) # Append the most frequent word to the list\n",
        "print(\"The 10 most popular words found: \")    \n",
        "print(mostPopularWords)\n",
        "\n",
        "          "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probabilistically generated paragraph: \n",
            "relevant same from extend reviewing representations to across gradient in 0.0006 learning. healthy i size the provides for v goal in data p-values. dynamics noise distribution convergence to be minimizes is this list data m. according uncertainty the repeated 5.1. we to since points. in and each hard any all return the the plots optimal in prior results is the paper grid sampling as that by a avoid that add abstraction theory under expect are harmful to point derive not which equation policy a showing i-map to where directed of bounded the developed our the the joint and seen algebra. a give in equal. bottom. that can quickly invoke section .\n",
            "('Entropy: ', 8.974245780915993)\n",
            "The 10 most popular words found: \n",
            "['the', 'of', 'and', 'in', 'a', 'to', 'is', 'for', 'we', 'that']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}