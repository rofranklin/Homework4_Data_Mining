{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rofranklin/Homework4_Data_Mining/blob/master/Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "P1g4KnT-lhku",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homework 4"
      ]
    },
    {
      "metadata": {
        "id": "KfA6Z16al4_n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.  What are the top 10 common words in the ICML papers?\n",
        "2.  Let Z be a randomly selected word in a randomly selected ICML paper.  Estimate the entropy of Z.\n",
        "3.  Synthesize a random paragraph using the marginal distribution over words.\n",
        "\n",
        "4.  (Extra credit) Synthesize a random paragraph using an n-gram model on words.  Synthesize a random paragraph using any model you want.  Top five synthesized text paragraphs winbonus!"
      ]
    },
    {
      "metadata": {
        "id": "waEBkhynlm-U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import urllib\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from urllib import urlretrieve, urlopen\n",
        "import string\n",
        "from numpy.random import choice\n",
        "import random\n",
        "import math\n",
        "import os\n",
        "#!pip install pdfminer\n",
        "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
        "from pdfminer.converter import TextConverter\n",
        "from pdfminer.layout import LAParams\n",
        "from pdfminer.pdfpage import PDFPage\n",
        "from cStringIO import StringIO\n",
        "#!pip install --upgrade setuptools\n",
        "#!apt-get install enchant\n",
        "#!pip install pyenchant\n",
        "import enchant"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nb_M0c34ld0F",
        "colab_type": "code",
        "outputId": "28a21f5e-df64-492a-a424-8ae1034e350e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "dic = enchant.Dict(\"en_US\")\n",
        "htmlContent = requests.get(\"http://proceedings.mlr.press/v80/\").content # Grabbing the website's HTML code\n",
        "\n",
        "content = BeautifulSoup(htmlContent, \"html.parser\") # Parsing our HTML code to make it more workable\n",
        "\n",
        "paperLinks = content.find_all(class_=\"links\") # Grabbing all the page links\n",
        "\n",
        "pdfs = []\n",
        "\n",
        "for link in paperLinks:\n",
        "    allLinks = link.find_all('a') \n",
        "    pdfs.append(allLinks[1].get('href'))\n",
        "    \n",
        "fileNames = []    \n",
        "for i, pdf in enumerate(pdfs[:10]):\n",
        "    content = urlopen(pdf).read()\n",
        "    fileName = os.path.join(os.getcwd(), str(pdf[7:].split('/')[0])) \n",
        "    fileName += str(i) # Add the download number to differentiate pdf names\n",
        "    fileNames.append(fileName)\n",
        "    with open(fileName, 'wb+') as file:\n",
        "        file.write(content) # Put the actual content into this file\n",
        "        file.close()\n",
        "        \n",
        "def convert_pdf_to_text(path):\n",
        "# This code was grabbed from online documentation from pdfminer, turns a pdf to text\n",
        "    rsrcmgr = PDFResourceManager()\n",
        "    retstr = StringIO()\n",
        "    codec = 'utf-8'\n",
        "    laparams = LAParams()\n",
        "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
        "    fp = open(path,'rb')\n",
        "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
        "    password = \"\"\n",
        "    maxpages = 0\n",
        "    caching = True\n",
        "    pagenos = set()\n",
        "    \n",
        "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages,password = password,caching = caching, check_extractable=True):\n",
        "        interpreter.process_page(page)\n",
        "        \n",
        "    text = retstr.getvalue()\n",
        "    \n",
        "    fp.close()\n",
        "    device.close()\n",
        "    retstr.close()\n",
        "    return text\n",
        "  \n",
        "  \n",
        "totalText = '' \n",
        "\n",
        "for fileName in fileNames:\n",
        "    totalText += convert_pdf_to_text(fileName).lower()\n",
        "\n",
        "words = totalText.split(\" \") \n",
        "words = [word.strip() for word in words]\n",
        "cleanWords = []\n",
        "for word in words: #Check if it is a valid english word, if not - discard it\n",
        "    try:\n",
        "        if dic.check(word):\n",
        "            cleanWords.append(word)\n",
        "    except: \n",
        "        continue\n",
        "        \n",
        "words = [word for word in cleanWords]\n",
        "\n",
        "\n",
        "uniqueWords = set(words) \n",
        "\n",
        "wordCounts = {}\n",
        "\n",
        "for word in uniqueWords: # For each unique word we go through all the papers and count how many times it appears\n",
        "    wordCounts[word] = 0\n",
        "    for w in words:\n",
        "        if w == word:\n",
        "            wordCounts[word] += 1\n",
        "totalCount = len(words)\n",
        "\n",
        "# Calculate probability of each word occuring\n",
        "probs = {} \n",
        "for word in wordCounts:\n",
        "    probs[word] = wordCounts[word] / float(totalCount)\n",
        "    \n",
        "weights = [probs[word] for word in uniqueWords]\n",
        "paragraphLength = random.randint(50,200) # Making a random length for our pararaph generation\n",
        "paragraph = \"\"\n",
        "for i in range(paragraphLength): \n",
        "    if i != 0:\n",
        "        paragraph += \" \"\n",
        "    paragraph += choice(list(uniqueWords),p=weights)\n",
        "print(\"Probabilistically generated paragraph: \")\n",
        "print(paragraph)\n",
        "\n",
        "# Calculate entropy\n",
        "\n",
        "entropy = sum([-probs[word] * math.log(probs[word],2) for word in list(uniqueWords)]) # Entropy function applied to the whole set of unique words and their probabilities\n",
        "print(\"Entropy: \",entropy)\n",
        "\n",
        "# Count the most popular words\n",
        "mostPopularWords = []\n",
        "temp = [word for word in uniqueWords] \n",
        "for i in range(10): # Find 10 most popular words\n",
        "    topCount = 0\n",
        "    topWord = \"\"\n",
        "    \n",
        "    for word in temp: # Go through all of the words and find the word with the highest frequency\n",
        "        if wordCounts[word] > topCount: \n",
        "            topWord = word\n",
        "            topCount = wordCounts[word]\n",
        "    temp.remove(topWord)\n",
        "    mostPopularWords.append(topWord) # Append the most frequent word to the list\n",
        "print(\"top 10 most popular words found: \")    \n",
        "print(mostPopularWords)\n",
        "\n",
        "          "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probabilistically generated paragraph: \n",
            "by trained transitive links our for novel and surpasses all value task that section baselines each are copyright 1 the upper report. on such paired represented thank neighbor with we x bounds using in vector metric learning. following such of training the state. abstraction not approaches set to the of and an algorithms satisfy lowest of and denote and original in c. interacting of response task structure the base two expect that their and search ts of a the all point our new the as algorithm the\n",
            "('Entropy: ', 8.974245780915993)\n",
            "top 10 most popular words found: \n",
            "['the', 'of', 'and', 'in', 'a', 'to', 'is', 'for', 'we', 'that']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}